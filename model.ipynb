{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e6e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from utils import load_json, dump_json\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc\n",
    "from torch.utils.tensorboard import SummaryWriter  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86216f",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691185d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datas = load_json(\"ps_sents_pair_with_label.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fcbbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle datas\n",
    "datas = shuffle(shuffle(datas,random_state=22),random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c3f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(datas,test_size=0.2, random_state=2022)\n",
    "test,val = train_test_split(test,test_size=0.5, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b974febe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent1': \"These achievements combined with academic progress resulted in the honour of signing the Head Teacher's Book, thirteen times.\",\n",
       " 'sent2': 'Since then I have been determined to not only help people in pain, I want to have the independence of making my own decisions and building friendly relationships with my clients.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb64c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1,train0,test1,test0,val1,val0=0,0,0,0,0,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277b8630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26870, 3359, 3359)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test),len(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03d998",
   "metadata": {},
   "source": [
    "# Load the BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fa789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True,cache_dir=os.path.join(os.getcwd(),\"bert-base-uncased_cache\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bb827",
   "metadata": {},
   "source": [
    "# find the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ae2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  141\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents1 = [d[\"sent1\"].lower() for d in datas]\n",
    "sents2 = [d[\"sent2\"].lower() for d in datas]\n",
    "sents = list(set(sents1+sents2))\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in sents]\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d4f5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a MAX_LEN you like\n",
    "MAX_LEN=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f53f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e80089",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95dec116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():       \n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "        print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b336f",
   "metadata": {},
   "source": [
    "# Create a function to tokenize a set of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868ce9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent[\"sent1\"].lower()),#The first sequence to be encoded\n",
    "            text_pair=text_preprocessing(sent[\"sent2\"].lower()),# second sequence to be encoded\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "#             return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9442d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " tensor([0, 0, 0,  ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert  labels to torch.Tensor\n",
    "train_labels = torch.tensor([t[\"label\"] for t in train])\n",
    "test_labels = torch.tensor([t[\"label\"] for t in test])\n",
    "val_labels = torch.tensor([t[\"label\"] for t in val])\n",
    "train_labels,test_labels,val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba356bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5589, 21281)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(train_labels).item(),len(train_labels)-torch.count_nonzero(train_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a63db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 2677)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(test_labels).item(),len(test_labels)-torch.count_nonzero(test_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a78d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 2638)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(val_labels).item(),len(val_labels)-torch.count_nonzero(val_labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a233efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2184: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "train_inputs, train_masks = preprocessing_for_bert(train)\n",
    "test_inputs, test_masks = preprocessing_for_bert(test)\n",
    "val_inputs, val_masks = preprocessing_for_bert(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa566f8e",
   "metadata": {},
   "source": [
    "# Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c262dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PyTorch DataLoader\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 4\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66892914",
   "metadata": {},
   "source": [
    "# Create BertClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe2e86",
   "metadata": {},
   "source": [
    "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the [CLS] token is used as the features of the sequence to feed a classifier.\n",
    "\n",
    "The transformers library has the BertForSequenceClassification class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
    "\n",
    "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7527dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 128, 2\n",
    "        unfreeze_layers = ['layer.10','layer.11','bert.pooler','out.']\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',cache_dir=os.path.join(os.getcwd(),\"bert-base-uncased_cache\"))\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "#             for name, param in self.bert.named_parameters():\n",
    "#                 print(name,param.size())\n",
    " \n",
    "#             print(\"*\"*30)\n",
    "#             print('\\n')\n",
    " \n",
    "            for name ,param in self.bert.named_parameters():\n",
    "                param.requires_grad = False\n",
    "                for ele in unfreeze_layers:\n",
    "                    if ele in name:\n",
    "                        param.requires_grad = True\n",
    "                        break\n",
    "#             #验证一下\n",
    "#             for name, param in self.bert.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print(name,param.size())\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27301f",
   "metadata": {},
   "source": [
    "# Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636af2a8",
   "metadata": {},
   "source": [
    "recommend following hyper-parameters\n",
    "   - Batch size: 16 or 32       \n",
    "   - Learning rate (Adam): 5e-5, 3e-5 or 2e-5  \n",
    "   - Number of epochs: 2, 3, 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79c2b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_train_step:  6718\n"
     ]
    }
   ],
   "source": [
    "len_train_step = len(train_dataloader)\n",
    "print('len_train_step: ',len_train_step)\n",
    "\n",
    "def initialize_model(epochs=10,fp16=False,n_gpu=0,local_rank=-1):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=True)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=1e-6,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len_train_step * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=int(total_steps/2), # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    #### Optional configuration\n",
    "    if fp16:\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\"\n",
    "        try:\n",
    "            from apex import amp\n",
    "            fp16_opt_level = '01'\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from \"\n",
    "            \"https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        bert_classifier, optimizer = amp.initialize(\n",
    "            bert_classifier, optimizer, opt_level=fp16_opt_level)\n",
    "        \"For fp16: Apex AMP optimization level \"\n",
    "        \"selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "        \"See details at https://nvidia.github.io/apex/amp.html\"\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if n_gpu > 1:\n",
    "        bert_classifier = torch.nn.DataParallel(bert_classifier)\n",
    "        \n",
    "     # Distributed training (should be after apex fp16 initialization)\n",
    "    if local_rank != -1:\n",
    "        bert_classifier = torch.nn.parallel.DistributedDataParallel(\n",
    "            bert_classifier, \n",
    "            device_ids=[local_rank],\n",
    "            output_device=local_rank,\n",
    "            find_unused_parameters=True\n",
    "            )\n",
    "        \n",
    "    return bert_classifier, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a2c2a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd3187",
   "metadata": {},
   "source": [
    "Training:\n",
    "\n",
    "- Unpack our data from the dataloader and load the data onto the GPU\n",
    "- Zero out gradients calculated in the previous pass\n",
    "- Perform a forward pass to compute logits and loss\n",
    "- Perform a backward pass to compute gradients (loss.backward())\n",
    "- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "- Update the model's parameters (optimizer.step())\n",
    "- Update the learning rate (scheduler.step())\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "- Unpack our data and load onto the GPU\n",
    "- Forward pass\n",
    "- Compute loss and accuracy rate over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28d5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./log/lr=1e-6droupout=0.0warmup=0.5batchsize=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c482b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='mean',label_smoothing=0.05)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    one_cricle_length = len(train_dataloader)\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed/lr':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            #print('logits,b_labels')\n",
    "            #print(logits)\n",
    "            #print(b_labels)\n",
    "            '''tensor([[-0.0491,  0.1918],\n",
    "                       [-0.0062,  0.1743],\n",
    "                       [-0.0856,  0.2257],\n",
    "                       [-0.0044,  0.1629]], grad_fn=<AddmmBackward>)\n",
    "                tensor([0, 1, 0, 0])\n",
    "            '''\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            #print(loss)\n",
    "            #print(logits.shape,b_labels.shape,loss.shape)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "#             b = 0.64\n",
    "#             flood = (loss-b).abs()+b\n",
    "#             flood.backward()\n",
    "            \n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\",L2 norm fuc\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0, norm_type=2)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            op_params =optimizer.state_dict()['param_groups'][0]\n",
    "            lr = op_params.get('lr')\n",
    "            \n",
    "            ########################################################################\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                writer.add_scalar('lr=1e-6droupout=0.0warmup=0.5batchsize=4/train_loss_step', batch_loss / batch_counts, step + epoch_i*one_cricle_length)\n",
    "                writer.add_scalar('lr=1e-6droupout=0.0warmup=0.5batchsize=4/train_lr_step', lr, step + epoch_i*one_cricle_length)\n",
    "\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "#                 writer.add_scalar('steploss/train_loss_step', batch_loss / batch_counts, step)\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {lr}\")\n",
    "                \n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "            ###############################################################\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_f1 = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            writer.add_scalar('lr=1e-6droupout=0.0warmup=0.5batchsize=4/train_loss', avg_train_loss, epoch_i)\n",
    "            writer.add_scalar('lr=1e-6droupout=0.0warmup=0.5batchsize=4/val_loss', val_loss, epoch_i)\n",
    "            writer.add_scalar('lr=1e-6droupout=0.0warmup=0.5batchsize=4val_f1', val_f1, epoch_i)\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_f1:^9.2f} | {time_elapsed:^15.6f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    writer.close()\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_f1 = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        # accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "#         accuracy = (preds == b_labels).cpu().numpy().mean()\n",
    "        f1 = f1_score(b_labels, preds, average='weighted')\n",
    "        val_f1.append(f1)\n",
    "#         val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_f1 = np.mean(val_f1)\n",
    "\n",
    "    return val_loss, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad9dfa",
   "metadata": {},
   "source": [
    "# running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200b9ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  | Elapsed/lr\n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.761838   |     -      |     -     | 5.954153021732658e-10\n",
      "   1    |   40    |   0.763377   |     -      |     -     | 1.1908306043465317e-09\n",
      "   1    |   60    |   0.777186   |     -      |     -     | 1.7862459065197973e-09\n",
      "   1    |   80    |   0.765086   |     -      |     -     | 2.3816612086930634e-09\n",
      "   1    |   100   |   0.750388   |     -      |     -     | 2.977076510866329e-09\n",
      "   1    |   120   |   0.755693   |     -      |     -     | 3.5724918130395946e-09\n",
      "   1    |   140   |   0.748822   |     -      |     -     | 4.167907115212861e-09\n",
      "   1    |   160   |   0.762722   |     -      |     -     | 4.763322417386127e-09\n",
      "   1    |   180   |   0.765131   |     -      |     -     | 5.358737719559392e-09\n",
      "   1    |   200   |   0.765088   |     -      |     -     | 5.954153021732658e-09\n",
      "   1    |   220   |   0.775377   |     -      |     -     | 6.549568323905924e-09\n",
      "   1    |   240   |   0.754932   |     -      |     -     | 7.144983626079189e-09\n",
      "   1    |   260   |   0.765752   |     -      |     -     | 7.740398928252455e-09\n",
      "   1    |   280   |   0.753205   |     -      |     -     | 8.335814230425723e-09\n",
      "   1    |   300   |   0.744057   |     -      |     -     | 8.931229532598987e-09\n",
      "   1    |   320   |   0.763116   |     -      |     -     | 9.526644834772253e-09\n",
      "   1    |   340   |   0.767475   |     -      |     -     | 1.012206013694552e-08\n",
      "   1    |   360   |   0.734881   |     -      |     -     | 1.0717475439118784e-08\n",
      "   1    |   380   |   0.753538   |     -      |     -     | 1.131289074129205e-08\n",
      "   1    |   400   |   0.766243   |     -      |     -     | 1.1908306043465317e-08\n",
      "   1    |   420   |   0.732614   |     -      |     -     | 1.2503721345638581e-08\n",
      "   1    |   440   |   0.760340   |     -      |     -     | 1.3099136647811848e-08\n",
      "   1    |   460   |   0.777718   |     -      |     -     | 1.3694551949985114e-08\n",
      "   1    |   480   |   0.764221   |     -      |     -     | 1.4289967252158378e-08\n",
      "   1    |   500   |   0.757326   |     -      |     -     | 1.4885382554331645e-08\n",
      "   1    |   520   |   0.741856   |     -      |     -     | 1.548079785650491e-08\n",
      "   1    |   540   |   0.736974   |     -      |     -     | 1.6076213158678176e-08\n",
      "   1    |   560   |   0.758389   |     -      |     -     | 1.6671628460851445e-08\n",
      "   1    |   580   |   0.748616   |     -      |     -     | 1.7267043763024708e-08\n",
      "   1    |   600   |   0.747842   |     -      |     -     | 1.7862459065197974e-08\n",
      "   1    |   620   |   0.768256   |     -      |     -     | 1.845787436737124e-08\n",
      "   1    |   640   |   0.760182   |     -      |     -     | 1.9053289669544507e-08\n",
      "   1    |   660   |   0.760073   |     -      |     -     | 1.964870497171777e-08\n",
      "   1    |   680   |   0.745596   |     -      |     -     | 2.024412027389104e-08\n",
      "   1    |   700   |   0.754686   |     -      |     -     | 2.0839535576064302e-08\n",
      "   1    |   720   |   0.768853   |     -      |     -     | 2.143495087823757e-08\n",
      "   1    |   740   |   0.746066   |     -      |     -     | 2.2030366180410838e-08\n",
      "   1    |   760   |   0.745442   |     -      |     -     | 2.26257814825841e-08\n",
      "   1    |   780   |   0.747182   |     -      |     -     | 2.3221196784757367e-08\n",
      "   1    |   800   |   0.750176   |     -      |     -     | 2.3816612086930634e-08\n",
      "   1    |   820   |   0.761397   |     -      |     -     | 2.44120273891039e-08\n",
      "   1    |   840   |   0.748167   |     -      |     -     | 2.5007442691277163e-08\n",
      "   1    |   860   |   0.752146   |     -      |     -     | 2.5602857993450432e-08\n",
      "   1    |   880   |   0.744853   |     -      |     -     | 2.6198273295623695e-08\n",
      "   1    |   900   |   0.758139   |     -      |     -     | 2.679368859779696e-08\n",
      "   1    |   920   |   0.749399   |     -      |     -     | 2.7389103899970228e-08\n",
      "   1    |   940   |   0.746059   |     -      |     -     | 2.7984519202143494e-08\n",
      "   1    |   960   |   0.733963   |     -      |     -     | 2.8579934504316757e-08\n",
      "   1    |   980   |   0.741040   |     -      |     -     | 2.9175349806490027e-08\n",
      "   1    |  1000   |   0.749386   |     -      |     -     | 2.977076510866329e-08\n",
      "   1    |  1020   |   0.748723   |     -      |     -     | 3.0366180410836556e-08\n",
      "   1    |  1040   |   0.748995   |     -      |     -     | 3.096159571300982e-08\n",
      "   1    |  1060   |   0.737569   |     -      |     -     | 3.155701101518309e-08\n",
      "   1    |  1080   |   0.745280   |     -      |     -     | 3.215242631735635e-08\n",
      "   1    |  1100   |   0.727871   |     -      |     -     | 3.274784161952962e-08\n",
      "   1    |  1120   |   0.742401   |     -      |     -     | 3.334325692170289e-08\n",
      "   1    |  1140   |   0.734171   |     -      |     -     | 3.393867222387615e-08\n",
      "   1    |  1160   |   0.743545   |     -      |     -     | 3.4534087526049416e-08\n",
      "   1    |  1180   |   0.736801   |     -      |     -     | 3.5129502828222686e-08\n",
      "   1    |  1200   |   0.739186   |     -      |     -     | 3.572491813039595e-08\n",
      "   1    |  1220   |   0.744415   |     -      |     -     | 3.632033343256922e-08\n",
      "   1    |  1240   |   0.736179   |     -      |     -     | 3.691574873474248e-08\n",
      "   1    |  1260   |   0.736300   |     -      |     -     | 3.7511164036915744e-08\n",
      "   1    |  1280   |   0.747285   |     -      |     -     | 3.8106579339089014e-08\n",
      "   1    |  1300   |   0.732211   |     -      |     -     | 3.870199464126228e-08\n",
      "   1    |  1320   |   0.734651   |     -      |     -     | 3.929740994343554e-08\n",
      "   1    |  1340   |   0.751401   |     -      |     -     | 3.989282524560881e-08\n",
      "   1    |  1360   |   0.732316   |     -      |     -     | 4.048824054778208e-08\n",
      "   1    |  1380   |   0.731750   |     -      |     -     | 4.108365584995534e-08\n",
      "   1    |  1400   |   0.739108   |     -      |     -     | 4.1679071152128605e-08\n",
      "   1    |  1420   |   0.743404   |     -      |     -     | 4.2274486454301874e-08\n",
      "   1    |  1440   |   0.734822   |     -      |     -     | 4.286990175647514e-08\n",
      "   1    |  1460   |   0.719633   |     -      |     -     | 4.3465317058648407e-08\n",
      "   1    |  1480   |   0.730559   |     -      |     -     | 4.4060732360821676e-08\n",
      "   1    |  1500   |   0.732854   |     -      |     -     | 4.465614766299493e-08\n",
      "   1    |  1520   |   0.737677   |     -      |     -     | 4.52515629651682e-08\n",
      "   1    |  1540   |   0.731662   |     -      |     -     | 4.584697826734147e-08\n",
      "   1    |  1560   |   0.722007   |     -      |     -     | 4.6442393569514735e-08\n",
      "   1    |  1580   |   0.737232   |     -      |     -     | 4.7037808871688e-08\n",
      "   1    |  1600   |   0.733250   |     -      |     -     | 4.763322417386127e-08\n",
      "   1    |  1620   |   0.727326   |     -      |     -     | 4.822863947603453e-08\n",
      "   1    |  1640   |   0.730868   |     -      |     -     | 4.88240547782078e-08\n",
      "   1    |  1660   |   0.739340   |     -      |     -     | 4.941947008038106e-08\n",
      "   1    |  1680   |   0.735138   |     -      |     -     | 5.0014885382554326e-08\n",
      "   1    |  1700   |   0.730886   |     -      |     -     | 5.0610300684727595e-08\n",
      "   1    |  1720   |   0.728380   |     -      |     -     | 5.1205715986900865e-08\n",
      "   1    |  1740   |   0.723922   |     -      |     -     | 5.180113128907412e-08\n",
      "   1    |  1760   |   0.723162   |     -      |     -     | 5.239654659124739e-08\n",
      "   1    |  1780   |   0.723965   |     -      |     -     | 5.299196189342066e-08\n",
      "   1    |  1800   |   0.720729   |     -      |     -     | 5.358737719559392e-08\n",
      "   1    |  1820   |   0.713660   |     -      |     -     | 5.418279249776719e-08\n",
      "   1    |  1840   |   0.715543   |     -      |     -     | 5.4778207799940456e-08\n",
      "   1    |  1860   |   0.714995   |     -      |     -     | 5.537362310211372e-08\n",
      "   1    |  1880   |   0.722759   |     -      |     -     | 5.596903840428699e-08\n",
      "   1    |  1900   |   0.716139   |     -      |     -     | 5.656445370646026e-08\n",
      "   1    |  1920   |   0.715983   |     -      |     -     | 5.7159869008633514e-08\n",
      "   1    |  1940   |   0.720261   |     -      |     -     | 5.7755284310806783e-08\n",
      "   1    |  1960   |   0.712907   |     -      |     -     | 5.835069961298005e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  1980   |   0.706651   |     -      |     -     | 5.894611491515332e-08\n",
      "   1    |  2000   |   0.714023   |     -      |     -     | 5.954153021732658e-08\n",
      "   1    |  2020   |   0.713647   |     -      |     -     | 6.013694551949984e-08\n",
      "   1    |  2040   |   0.717290   |     -      |     -     | 6.073236082167311e-08\n",
      "   1    |  2060   |   0.711208   |     -      |     -     | 6.132777612384638e-08\n",
      "   1    |  2080   |   0.705910   |     -      |     -     | 6.192319142601964e-08\n",
      "   1    |  2100   |   0.710987   |     -      |     -     | 6.251860672819291e-08\n",
      "   1    |  2120   |   0.707816   |     -      |     -     | 6.311402203036618e-08\n",
      "   1    |  2140   |   0.705626   |     -      |     -     | 6.370943733253945e-08\n",
      "   1    |  2160   |   0.708541   |     -      |     -     | 6.43048526347127e-08\n",
      "   1    |  2180   |   0.700679   |     -      |     -     | 6.490026793688599e-08\n",
      "   1    |  2200   |   0.709295   |     -      |     -     | 6.549568323905924e-08\n",
      "   1    |  2220   |   0.707684   |     -      |     -     | 6.60910985412325e-08\n",
      "   1    |  2240   |   0.702864   |     -      |     -     | 6.668651384340578e-08\n",
      "   1    |  2260   |   0.701993   |     -      |     -     | 6.728192914557904e-08\n",
      "   1    |  2280   |   0.702123   |     -      |     -     | 6.78773444477523e-08\n",
      "   1    |  2300   |   0.702789   |     -      |     -     | 6.847275974992558e-08\n",
      "   1    |  2320   |   0.702581   |     -      |     -     | 6.906817505209883e-08\n",
      "   1    |  2340   |   0.696362   |     -      |     -     | 6.96635903542721e-08\n",
      "   1    |  2360   |   0.695449   |     -      |     -     | 7.025900565644537e-08\n",
      "   1    |  2380   |   0.695020   |     -      |     -     | 7.085442095861863e-08\n",
      "   1    |  2400   |   0.697071   |     -      |     -     | 7.14498362607919e-08\n",
      "   1    |  2420   |   0.692133   |     -      |     -     | 7.204525156296517e-08\n",
      "   1    |  2440   |   0.691177   |     -      |     -     | 7.264066686513844e-08\n",
      "   1    |  2460   |   0.698482   |     -      |     -     | 7.323608216731169e-08\n",
      "   1    |  2480   |   0.681798   |     -      |     -     | 7.383149746948496e-08\n",
      "   1    |  2500   |   0.691663   |     -      |     -     | 7.442691277165823e-08\n",
      "   1    |  2520   |   0.693266   |     -      |     -     | 7.502232807383149e-08\n",
      "   1    |  2540   |   0.689372   |     -      |     -     | 7.561774337600477e-08\n",
      "   1    |  2560   |   0.687710   |     -      |     -     | 7.621315867817803e-08\n",
      "   1    |  2580   |   0.686670   |     -      |     -     | 7.680857398035128e-08\n",
      "   1    |  2600   |   0.686045   |     -      |     -     | 7.740398928252457e-08\n",
      "   1    |  2620   |   0.687627   |     -      |     -     | 7.799940458469782e-08\n",
      "   1    |  2640   |   0.691114   |     -      |     -     | 7.859481988687108e-08\n",
      "   1    |  2660   |   0.685052   |     -      |     -     | 7.919023518904436e-08\n",
      "   1    |  2680   |   0.686157   |     -      |     -     | 7.978565049121762e-08\n",
      "   1    |  2700   |   0.686329   |     -      |     -     | 8.038106579339089e-08\n",
      "   1    |  2720   |   0.676768   |     -      |     -     | 8.097648109556416e-08\n",
      "   1    |  2740   |   0.683854   |     -      |     -     | 8.157189639773741e-08\n",
      "   1    |  2760   |   0.680094   |     -      |     -     | 8.216731169991068e-08\n",
      "   1    |  2780   |   0.671840   |     -      |     -     | 8.276272700208395e-08\n",
      "   1    |  2800   |   0.680587   |     -      |     -     | 8.335814230425721e-08\n",
      "   1    |  2820   |   0.675177   |     -      |     -     | 8.395355760643048e-08\n",
      "   1    |  2840   |   0.678162   |     -      |     -     | 8.454897290860375e-08\n",
      "   1    |  2860   |   0.670596   |     -      |     -     | 8.514438821077702e-08\n",
      "   1    |  2880   |   0.671190   |     -      |     -     | 8.573980351295027e-08\n",
      "   1    |  2900   |   0.667231   |     -      |     -     | 8.633521881512354e-08\n",
      "   1    |  2920   |   0.665200   |     -      |     -     | 8.693063411729681e-08\n",
      "   1    |  2940   |   0.670798   |     -      |     -     | 8.752604941947007e-08\n",
      "   1    |  2960   |   0.659680   |     -      |     -     | 8.812146472164335e-08\n",
      "   1    |  2980   |   0.657705   |     -      |     -     | 8.871688002381661e-08\n",
      "   1    |  3000   |   0.666662   |     -      |     -     | 8.931229532598987e-08\n",
      "   1    |  3020   |   0.661929   |     -      |     -     | 8.990771062816315e-08\n",
      "   1    |  3040   |   0.671233   |     -      |     -     | 9.05031259303364e-08\n",
      "   1    |  3060   |   0.649802   |     -      |     -     | 9.109854123250966e-08\n",
      "   1    |  3080   |   0.662125   |     -      |     -     | 9.169395653468294e-08\n",
      "   1    |  3100   |   0.653405   |     -      |     -     | 9.22893718368562e-08\n",
      "   1    |  3120   |   0.657713   |     -      |     -     | 9.288478713902947e-08\n",
      "   1    |  3140   |   0.652738   |     -      |     -     | 9.348020244120274e-08\n",
      "   1    |  3160   |   0.656918   |     -      |     -     | 9.4075617743376e-08\n",
      "   1    |  3180   |   0.658774   |     -      |     -     | 9.467103304554926e-08\n",
      "   1    |  3200   |   0.651623   |     -      |     -     | 9.526644834772253e-08\n",
      "   1    |  3220   |   0.662875   |     -      |     -     | 9.586186364989579e-08\n",
      "   1    |  3240   |   0.660180   |     -      |     -     | 9.645727895206906e-08\n",
      "   1    |  3260   |   0.667095   |     -      |     -     | 9.705269425424233e-08\n",
      "   1    |  3280   |   0.641563   |     -      |     -     | 9.76481095564156e-08\n",
      "   1    |  3300   |   0.653384   |     -      |     -     | 9.824352485858886e-08\n",
      "   1    |  3320   |   0.651636   |     -      |     -     | 9.883894016076213e-08\n",
      "   1    |  3340   |   0.643091   |     -      |     -     | 9.94343554629354e-08\n",
      "   1    |  3360   |   0.646687   |     -      |     -     | 1.0002977076510865e-07\n",
      "   1    |  3380   |   0.659767   |     -      |     -     | 1.0062518606728193e-07\n",
      "   1    |  3400   |   0.648334   |     -      |     -     | 1.0122060136945519e-07\n",
      "   1    |  3420   |   0.642468   |     -      |     -     | 1.0181601667162845e-07\n",
      "   1    |  3440   |   0.654050   |     -      |     -     | 1.0241143197380173e-07\n",
      "   1    |  3460   |   0.632895   |     -      |     -     | 1.0300684727597499e-07\n",
      "   1    |  3480   |   0.651129   |     -      |     -     | 1.0360226257814824e-07\n",
      "   1    |  3500   |   0.643332   |     -      |     -     | 1.0419767788032152e-07\n",
      "   1    |  3520   |   0.643547   |     -      |     -     | 1.0479309318249478e-07\n",
      "   1    |  3540   |   0.618658   |     -      |     -     | 1.0538850848466805e-07\n",
      "   1    |  3560   |   0.639497   |     -      |     -     | 1.0598392378684132e-07\n",
      "   1    |  3580   |   0.653159   |     -      |     -     | 1.0657933908901458e-07\n",
      "   1    |  3600   |   0.634698   |     -      |     -     | 1.0717475439118785e-07\n",
      "   1    |  3620   |   0.610761   |     -      |     -     | 1.0777016969336112e-07\n",
      "   1    |  3640   |   0.638363   |     -      |     -     | 1.0836558499553439e-07\n",
      "   1    |  3660   |   0.646609   |     -      |     -     | 1.0896100029770764e-07\n",
      "   1    |  3680   |   0.624262   |     -      |     -     | 1.0955641559988091e-07\n",
      "   1    |  3700   |   0.636538   |     -      |     -     | 1.1015183090205418e-07\n",
      "   1    |  3720   |   0.625758   |     -      |     -     | 1.1074724620422744e-07\n",
      "   1    |  3740   |   0.631740   |     -      |     -     | 1.113426615064007e-07\n",
      "   1    |  3760   |   0.625935   |     -      |     -     | 1.1193807680857398e-07\n",
      "   1    |  3780   |   0.623751   |     -      |     -     | 1.1253349211074723e-07\n",
      "   1    |  3800   |   0.621367   |     -      |     -     | 1.1312890741292052e-07\n",
      "   1    |  3820   |   0.625603   |     -      |     -     | 1.1372432271509377e-07\n",
      "   1    |  3840   |   0.606447   |     -      |     -     | 1.1431973801726703e-07\n",
      "   1    |  3860   |   0.630568   |     -      |     -     | 1.1491515331944031e-07\n",
      "   1    |  3880   |   0.629161   |     -      |     -     | 1.1551056862161357e-07\n",
      "   1    |  3900   |   0.621616   |     -      |     -     | 1.1610598392378682e-07\n",
      "   1    |  3920   |   0.607995   |     -      |     -     | 1.167013992259601e-07\n",
      "   1    |  3940   |   0.597931   |     -      |     -     | 1.1729681452813336e-07\n",
      "   1    |  3960   |   0.633066   |     -      |     -     | 1.1789222983030665e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  3980   |   0.663692   |     -      |     -     | 1.184876451324799e-07\n",
      "   1    |  4000   |   0.591198   |     -      |     -     | 1.1908306043465316e-07\n",
      "   1    |  4020   |   0.576435   |     -      |     -     | 1.1967847573682644e-07\n",
      "   1    |  4040   |   0.611475   |     -      |     -     | 1.2027389103899968e-07\n",
      "   1    |  4060   |   0.593397   |     -      |     -     | 1.2086930634117298e-07\n",
      "   1    |  4080   |   0.579902   |     -      |     -     | 1.2146472164334622e-07\n",
      "   1    |  4100   |   0.595288   |     -      |     -     | 1.220601369455195e-07\n",
      "   1    |  4120   |   0.577261   |     -      |     -     | 1.2265555224769276e-07\n",
      "   1    |  4140   |   0.619609   |     -      |     -     | 1.2325096754986603e-07\n",
      "   1    |  4160   |   0.596826   |     -      |     -     | 1.2384638285203927e-07\n",
      "   1    |  4180   |   0.618415   |     -      |     -     | 1.2444179815421257e-07\n",
      "   1    |  4200   |   0.578730   |     -      |     -     | 1.2503721345638581e-07\n",
      "   1    |  4220   |   0.579307   |     -      |     -     | 1.2563262875855908e-07\n",
      "   1    |  4240   |   0.577855   |     -      |     -     | 1.2622804406073235e-07\n",
      "   1    |  4260   |   0.596189   |     -      |     -     | 1.2682345936290562e-07\n",
      "   1    |  4280   |   0.602745   |     -      |     -     | 1.274188746650789e-07\n",
      "   1    |  4300   |   0.618052   |     -      |     -     | 1.2801428996725216e-07\n",
      "   1    |  4320   |   0.595628   |     -      |     -     | 1.286097052694254e-07\n",
      "   1    |  4340   |   0.580960   |     -      |     -     | 1.2920512057159867e-07\n",
      "   1    |  4360   |   0.611400   |     -      |     -     | 1.2980053587377197e-07\n",
      "   1    |  4380   |   0.570662   |     -      |     -     | 1.3039595117594521e-07\n",
      "   1    |  4400   |   0.614552   |     -      |     -     | 1.3099136647811848e-07\n",
      "   1    |  4420   |   0.578573   |     -      |     -     | 1.3158678178029175e-07\n",
      "   1    |  4440   |   0.605058   |     -      |     -     | 1.32182197082465e-07\n",
      "   1    |  4460   |   0.568528   |     -      |     -     | 1.3277761238463827e-07\n",
      "   1    |  4480   |   0.597202   |     -      |     -     | 1.3337302768681156e-07\n",
      "   1    |  4500   |   0.575025   |     -      |     -     | 1.339684429889848e-07\n",
      "   1    |  4520   |   0.569350   |     -      |     -     | 1.3456385829115807e-07\n",
      "   1    |  4540   |   0.568697   |     -      |     -     | 1.3515927359333134e-07\n",
      "   1    |  4560   |   0.546618   |     -      |     -     | 1.357546888955046e-07\n",
      "   1    |  4580   |   0.605788   |     -      |     -     | 1.3635010419767786e-07\n",
      "   1    |  4600   |   0.585673   |     -      |     -     | 1.3694551949985115e-07\n",
      "   1    |  4620   |   0.560746   |     -      |     -     | 1.3754093480202442e-07\n",
      "   1    |  4640   |   0.552010   |     -      |     -     | 1.3813635010419766e-07\n",
      "   1    |  4660   |   0.557154   |     -      |     -     | 1.3873176540637093e-07\n",
      "   1    |  4680   |   0.561626   |     -      |     -     | 1.393271807085442e-07\n",
      "   1    |  4700   |   0.580409   |     -      |     -     | 1.3992259601071745e-07\n",
      "   1    |  4720   |   0.587078   |     -      |     -     | 1.4051801131289074e-07\n",
      "   1    |  4740   |   0.556228   |     -      |     -     | 1.41113426615064e-07\n",
      "   1    |  4760   |   0.577924   |     -      |     -     | 1.4170884191723726e-07\n",
      "   1    |  4780   |   0.533621   |     -      |     -     | 1.4230425721941053e-07\n",
      "   1    |  4800   |   0.584983   |     -      |     -     | 1.428996725215838e-07\n",
      "   1    |  4820   |   0.549819   |     -      |     -     | 1.4349508782375706e-07\n",
      "   1    |  4840   |   0.529874   |     -      |     -     | 1.4409050312593033e-07\n",
      "   1    |  4860   |   0.593156   |     -      |     -     | 1.446859184281036e-07\n",
      "   1    |  4880   |   0.515567   |     -      |     -     | 1.4528133373027687e-07\n",
      "   1    |  4900   |   0.489986   |     -      |     -     | 1.4587674903245012e-07\n",
      "   1    |  4920   |   0.599443   |     -      |     -     | 1.4647216433462339e-07\n",
      "   1    |  4940   |   0.554805   |     -      |     -     | 1.4706757963679666e-07\n",
      "   1    |  4960   |   0.556595   |     -      |     -     | 1.4766299493896992e-07\n",
      "   1    |  4980   |   0.520740   |     -      |     -     | 1.482584102411432e-07\n",
      "   1    |  5000   |   0.514548   |     -      |     -     | 1.4885382554331646e-07\n",
      "   1    |  5020   |   0.580653   |     -      |     -     | 1.494492408454897e-07\n",
      "   1    |  5040   |   0.595571   |     -      |     -     | 1.5004465614766298e-07\n",
      "   1    |  5060   |   0.557200   |     -      |     -     | 1.5064007144983625e-07\n",
      "   1    |  5080   |   0.529932   |     -      |     -     | 1.5123548675200954e-07\n",
      "   1    |  5100   |   0.582893   |     -      |     -     | 1.5183090205418279e-07\n",
      "   1    |  5120   |   0.556903   |     -      |     -     | 1.5242631735635605e-07\n",
      "   1    |  5140   |   0.532805   |     -      |     -     | 1.5302173265852932e-07\n",
      "   1    |  5160   |   0.549353   |     -      |     -     | 1.5361714796070257e-07\n",
      "   1    |  5180   |   0.526022   |     -      |     -     | 1.5421256326287584e-07\n",
      "   1    |  5200   |   0.580734   |     -      |     -     | 1.5480797856504913e-07\n",
      "   1    |  5220   |   0.601605   |     -      |     -     | 1.5540339386722238e-07\n",
      "   1    |  5240   |   0.545647   |     -      |     -     | 1.5599880916939565e-07\n",
      "   1    |  5260   |   0.584930   |     -      |     -     | 1.5659422447156892e-07\n",
      "   1    |  5280   |   0.501478   |     -      |     -     | 1.5718963977374216e-07\n",
      "   1    |  5300   |   0.511317   |     -      |     -     | 1.5778505507591543e-07\n",
      "   1    |  5320   |   0.504078   |     -      |     -     | 1.5838047037808872e-07\n",
      "   1    |  5340   |   0.517793   |     -      |     -     | 1.5897588568026197e-07\n",
      "   1    |  5360   |   0.539271   |     -      |     -     | 1.5957130098243524e-07\n",
      "   1    |  5380   |   0.548101   |     -      |     -     | 1.601667162846085e-07\n",
      "   1    |  5400   |   0.528144   |     -      |     -     | 1.6076213158678178e-07\n",
      "   1    |  5420   |   0.565404   |     -      |     -     | 1.6135754688895502e-07\n",
      "   1    |  5440   |   0.550639   |     -      |     -     | 1.6195296219112831e-07\n",
      "   1    |  5460   |   0.530909   |     -      |     -     | 1.6254837749330158e-07\n",
      "   1    |  5480   |   0.526437   |     -      |     -     | 1.6314379279547483e-07\n",
      "   1    |  5500   |   0.614143   |     -      |     -     | 1.637392080976481e-07\n",
      "   1    |  5520   |   0.458536   |     -      |     -     | 1.6433462339982137e-07\n",
      "   1    |  5540   |   0.490599   |     -      |     -     | 1.649300387019946e-07\n",
      "   1    |  5560   |   0.522104   |     -      |     -     | 1.655254540041679e-07\n",
      "   1    |  5580   |   0.594485   |     -      |     -     | 1.6612086930634118e-07\n",
      "   1    |  5600   |   0.589881   |     -      |     -     | 1.6671628460851442e-07\n",
      "   1    |  5620   |   0.473760   |     -      |     -     | 1.673116999106877e-07\n",
      "   1    |  5640   |   0.493840   |     -      |     -     | 1.6790711521286096e-07\n",
      "   1    |  5660   |   0.518408   |     -      |     -     | 1.6850253051503425e-07\n",
      "   1    |  5680   |   0.517834   |     -      |     -     | 1.690979458172075e-07\n",
      "   1    |  5700   |   0.475615   |     -      |     -     | 1.6969336111938077e-07\n",
      "   1    |  5720   |   0.525706   |     -      |     -     | 1.7028877642155404e-07\n",
      "   1    |  5740   |   0.577393   |     -      |     -     | 1.7088419172372728e-07\n",
      "   1    |  5760   |   0.571846   |     -      |     -     | 1.7147960702590055e-07\n",
      "   1    |  5780   |   0.530621   |     -      |     -     | 1.7207502232807384e-07\n",
      "   1    |  5800   |   0.605731   |     -      |     -     | 1.726704376302471e-07\n",
      "   1    |  5820   |   0.456143   |     -      |     -     | 1.7326585293242036e-07\n",
      "   1    |  5840   |   0.547015   |     -      |     -     | 1.7386126823459363e-07\n",
      "   1    |  5860   |   0.502804   |     -      |     -     | 1.7445668353676687e-07\n",
      "   1    |  5880   |   0.544754   |     -      |     -     | 1.7505209883894014e-07\n",
      "   1    |  5900   |   0.503581   |     -      |     -     | 1.7564751414111344e-07\n",
      "   1    |  5920   |   0.523627   |     -      |     -     | 1.762429294432867e-07\n",
      "   1    |  5940   |   0.568030   |     -      |     -     | 1.7683834474545995e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  5960   |   0.520951   |     -      |     -     | 1.7743376004763322e-07\n",
      "   1    |  5980   |   0.499683   |     -      |     -     | 1.780291753498065e-07\n",
      "   1    |  6000   |   0.510507   |     -      |     -     | 1.7862459065197973e-07\n",
      "   1    |  6020   |   0.538781   |     -      |     -     | 1.7922000595415303e-07\n",
      "   1    |  6040   |   0.494235   |     -      |     -     | 1.798154212563263e-07\n",
      "   1    |  6060   |   0.541044   |     -      |     -     | 1.8041083655849954e-07\n",
      "   1    |  6080   |   0.488781   |     -      |     -     | 1.810062518606728e-07\n",
      "   1    |  6100   |   0.610495   |     -      |     -     | 1.8160166716284608e-07\n",
      "   1    |  6120   |   0.488126   |     -      |     -     | 1.8219708246501932e-07\n",
      "   1    |  6140   |   0.479988   |     -      |     -     | 1.8279249776719262e-07\n",
      "   1    |  6160   |   0.501548   |     -      |     -     | 1.8338791306936589e-07\n",
      "   1    |  6180   |   0.506272   |     -      |     -     | 1.8398332837153916e-07\n",
      "   1    |  6200   |   0.456302   |     -      |     -     | 1.845787436737124e-07\n",
      "   1    |  6220   |   0.500237   |     -      |     -     | 1.8517415897588567e-07\n",
      "   1    |  6240   |   0.442247   |     -      |     -     | 1.8576957427805894e-07\n",
      "   1    |  6260   |   0.517955   |     -      |     -     | 1.863649895802322e-07\n",
      "   1    |  6280   |   0.484400   |     -      |     -     | 1.8696040488240548e-07\n",
      "   1    |  6300   |   0.498363   |     -      |     -     | 1.8755582018457875e-07\n",
      "   1    |  6320   |   0.557454   |     -      |     -     | 1.88151235486752e-07\n",
      "   1    |  6340   |   0.459849   |     -      |     -     | 1.8874665078892526e-07\n",
      "   1    |  6360   |   0.518466   |     -      |     -     | 1.8934206609109853e-07\n",
      "   1    |  6380   |   0.431241   |     -      |     -     | 1.899374813932718e-07\n",
      "   1    |  6400   |   0.540664   |     -      |     -     | 1.9053289669544507e-07\n",
      "   1    |  6420   |   0.456757   |     -      |     -     | 1.9112831199761834e-07\n",
      "   1    |  6440   |   0.485806   |     -      |     -     | 1.9172372729979158e-07\n",
      "   1    |  6460   |   0.408203   |     -      |     -     | 1.9231914260196485e-07\n",
      "   1    |  6480   |   0.566593   |     -      |     -     | 1.9291455790413812e-07\n",
      "   1    |  6500   |   0.533847   |     -      |     -     | 1.9350997320631142e-07\n",
      "   1    |  6520   |   0.473684   |     -      |     -     | 1.9410538850848466e-07\n",
      "   1    |  6540   |   0.537947   |     -      |     -     | 1.9470080381065793e-07\n",
      "   1    |  6560   |   0.429435   |     -      |     -     | 1.952962191128312e-07\n",
      "   1    |  6580   |   0.530333   |     -      |     -     | 1.9589163441500444e-07\n",
      "   1    |  6600   |   0.459506   |     -      |     -     | 1.964870497171777e-07\n",
      "   1    |  6620   |   0.483723   |     -      |     -     | 1.97082465019351e-07\n",
      "   1    |  6640   |   0.425488   |     -      |     -     | 1.9767788032152425e-07\n",
      "   1    |  6660   |   0.487505   |     -      |     -     | 1.9827329562369752e-07\n",
      "   1    |  6680   |   0.477965   |     -      |     -     | 1.988687109258708e-07\n",
      "   1    |  6700   |   0.491038   |     -      |     -     | 1.9946412622804403e-07\n",
      "   1    |  6717   |   0.618991   |     -      |     -     | 1.999702292348913e-07\n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.638130   |  0.510796  |   0.71    |   4593.921272  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  | Elapsed/lr\n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.457833   |     -      |     -     | 2.0059541530217326e-07\n",
      "   2    |   40    |   0.417319   |     -      |     -     | 2.0119083060434653e-07\n",
      "   2    |   60    |   0.459403   |     -      |     -     | 2.017862459065198e-07\n",
      "   2    |   80    |   0.595294   |     -      |     -     | 2.0238166120869304e-07\n",
      "   2    |   100   |   0.560697   |     -      |     -     | 2.029770765108663e-07\n",
      "   2    |   120   |   0.454044   |     -      |     -     | 2.0357249181303958e-07\n",
      "   2    |   140   |   0.531217   |     -      |     -     | 2.0416790711521285e-07\n",
      "   2    |   160   |   0.508577   |     -      |     -     | 2.0476332241738612e-07\n",
      "   2    |   180   |   0.479432   |     -      |     -     | 2.053587377195594e-07\n",
      "   2    |   200   |   0.496020   |     -      |     -     | 2.0595415302173266e-07\n",
      "   2    |   220   |   0.387415   |     -      |     -     | 2.065495683239059e-07\n",
      "   2    |   240   |   0.435626   |     -      |     -     | 2.0714498362607917e-07\n",
      "   2    |   260   |   0.495327   |     -      |     -     | 2.0774039892825247e-07\n",
      "   2    |   280   |   0.490772   |     -      |     -     | 2.083358142304257e-07\n",
      "   2    |   300   |   0.496538   |     -      |     -     | 2.0893122953259898e-07\n",
      "   2    |   320   |   0.555193   |     -      |     -     | 2.0952664483477225e-07\n",
      "   2    |   340   |   0.449520   |     -      |     -     | 2.101220601369455e-07\n",
      "   2    |   360   |   0.415619   |     -      |     -     | 2.1071747543911876e-07\n",
      "   2    |   380   |   0.385493   |     -      |     -     | 2.1131289074129206e-07\n",
      "   2    |   400   |   0.509789   |     -      |     -     | 2.119083060434653e-07\n",
      "   2    |   420   |   0.512779   |     -      |     -     | 2.1250372134563857e-07\n",
      "   2    |   440   |   0.591853   |     -      |     -     | 2.1309913664781184e-07\n",
      "   2    |   460   |   0.591776   |     -      |     -     | 2.1369455194998508e-07\n",
      "   2    |   480   |   0.432057   |     -      |     -     | 2.1428996725215835e-07\n",
      "   2    |   500   |   0.510876   |     -      |     -     | 2.1488538255433165e-07\n",
      "   2    |   520   |   0.563519   |     -      |     -     | 2.1548079785650492e-07\n",
      "   2    |   540   |   0.610918   |     -      |     -     | 2.1607621315867816e-07\n",
      "   2    |   560   |   0.452962   |     -      |     -     | 2.1667162846085143e-07\n",
      "   2    |   580   |   0.547748   |     -      |     -     | 2.172670437630247e-07\n",
      "   2    |   600   |   0.556635   |     -      |     -     | 2.1786245906519795e-07\n",
      "   2    |   620   |   0.421245   |     -      |     -     | 2.1845787436737124e-07\n",
      "   2    |   640   |   0.536529   |     -      |     -     | 2.190532896695445e-07\n",
      "   2    |   660   |   0.500049   |     -      |     -     | 2.1964870497171775e-07\n",
      "   2    |   680   |   0.592986   |     -      |     -     | 2.2024412027389102e-07\n",
      "   2    |   700   |   0.500320   |     -      |     -     | 2.208395355760643e-07\n",
      "   2    |   720   |   0.456755   |     -      |     -     | 2.2143495087823754e-07\n",
      "   2    |   740   |   0.387584   |     -      |     -     | 2.2203036618041083e-07\n",
      "   2    |   760   |   0.402865   |     -      |     -     | 2.226257814825841e-07\n",
      "   2    |   780   |   0.418924   |     -      |     -     | 2.2322119678475737e-07\n",
      "   2    |   800   |   0.496040   |     -      |     -     | 2.2381661208693061e-07\n",
      "   2    |   820   |   0.599294   |     -      |     -     | 2.2441202738910388e-07\n",
      "   2    |   840   |   0.549826   |     -      |     -     | 2.2500744269127715e-07\n",
      "   2    |   860   |   0.527014   |     -      |     -     | 2.2560285799345042e-07\n",
      "   2    |   880   |   0.463999   |     -      |     -     | 2.261982732956237e-07\n",
      "   2    |   900   |   0.349214   |     -      |     -     | 2.2679368859779696e-07\n",
      "   2    |   920   |   0.416995   |     -      |     -     | 2.273891038999702e-07\n",
      "   2    |   940   |   0.526327   |     -      |     -     | 2.2798451920214348e-07\n",
      "   2    |   960   |   0.463496   |     -      |     -     | 2.2857993450431674e-07\n",
      "   2    |   980   |   0.567058   |     -      |     -     | 2.2917534980649001e-07\n",
      "   2    |  1000   |   0.631656   |     -      |     -     | 2.2977076510866328e-07\n",
      "   2    |  1020   |   0.452493   |     -      |     -     | 2.3036618041083655e-07\n",
      "   2    |  1040   |   0.583079   |     -      |     -     | 2.3096159571300982e-07\n",
      "   2    |  1060   |   0.571733   |     -      |     -     | 2.3155701101518307e-07\n",
      "   2    |  1080   |   0.513752   |     -      |     -     | 2.3215242631735634e-07\n",
      "   2    |  1100   |   0.483782   |     -      |     -     | 2.3274784161952963e-07\n",
      "   2    |  1120   |   0.598296   |     -      |     -     | 2.3334325692170287e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  1140   |   0.581667   |     -      |     -     | 2.3393867222387614e-07\n",
      "   2    |  1160   |   0.475587   |     -      |     -     | 2.3453408752604941e-07\n",
      "   2    |  1180   |   0.522047   |     -      |     -     | 2.3512950282822266e-07\n",
      "   2    |  1200   |   0.572367   |     -      |     -     | 2.3572491813039593e-07\n",
      "   2    |  1220   |   0.492985   |     -      |     -     | 2.3632033343256922e-07\n",
      "   2    |  1240   |   0.563165   |     -      |     -     | 2.3691574873474247e-07\n",
      "   2    |  1260   |   0.490675   |     -      |     -     | 2.3751116403691574e-07\n",
      "   2    |  1280   |   0.496382   |     -      |     -     | 2.38106579339089e-07\n",
      "   2    |  1300   |   0.487547   |     -      |     -     | 2.387019946412623e-07\n",
      "   2    |  1320   |   0.548495   |     -      |     -     | 2.392974099434355e-07\n",
      "   2    |  1340   |   0.563930   |     -      |     -     | 2.398928252456088e-07\n",
      "   2    |  1360   |   0.419023   |     -      |     -     | 2.4048824054778206e-07\n",
      "   2    |  1380   |   0.481113   |     -      |     -     | 2.410836558499553e-07\n",
      "   2    |  1400   |   0.425879   |     -      |     -     | 2.416790711521286e-07\n",
      "   2    |  1420   |   0.528772   |     -      |     -     | 2.4227448645430187e-07\n",
      "   2    |  1440   |   0.477795   |     -      |     -     | 2.4286990175647513e-07\n",
      "   2    |  1460   |   0.329863   |     -      |     -     | 2.434653170586484e-07\n",
      "   2    |  1480   |   0.482529   |     -      |     -     | 2.440607323608217e-07\n",
      "   2    |  1500   |   0.473153   |     -      |     -     | 2.446561476629949e-07\n",
      "   2    |  1520   |   0.607668   |     -      |     -     | 2.452515629651682e-07\n",
      "   2    |  1540   |   0.529768   |     -      |     -     | 2.458469782673415e-07\n",
      "   2    |  1560   |   0.612683   |     -      |     -     | 2.464423935695147e-07\n",
      "   2    |  1580   |   0.479280   |     -      |     -     | 2.4703780887168797e-07\n",
      "   2    |  1600   |   0.586580   |     -      |     -     | 2.4763322417386124e-07\n",
      "   2    |  1620   |   0.541675   |     -      |     -     | 2.4822863947603456e-07\n",
      "   2    |  1640   |   0.433403   |     -      |     -     | 2.488240547782078e-07\n",
      "   2    |  1660   |   0.506337   |     -      |     -     | 2.4941947008038105e-07\n",
      "   2    |  1680   |   0.430357   |     -      |     -     | 2.500148853825543e-07\n",
      "   2    |  1700   |   0.467540   |     -      |     -     | 2.506103006847276e-07\n",
      "   2    |  1720   |   0.552761   |     -      |     -     | 2.5120571598690086e-07\n",
      "   2    |  1740   |   0.504099   |     -      |     -     | 2.5180113128907407e-07\n",
      "   2    |  1760   |   0.611189   |     -      |     -     | 2.523965465912474e-07\n",
      "   2    |  1780   |   0.542655   |     -      |     -     | 2.5299196189342066e-07\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=epochs)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=epochs, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#saving a checkpoint assuming the network class named ClassNet\n",
    "checkpoint={'modle':bert_classifier,\n",
    "             'model_state_dict':bert_classifier.state_dict(),\n",
    "             'optimize_state_dict':optimizer.state_dict(),\n",
    "             'epoch':epochs}\n",
    "torch.save(checkpoint,'checkoutpoints/checkpoint_epoch_10_lr_1e-6_dropout_0_warmupstep_0.5_seed_42.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath,optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model=checkpoint['modle']#提前网络结构\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])#加载网络权重参数\n",
    "    optimizer=optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimize_state_dict'])#加载优化器参数\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad=False\n",
    "    model.eval()\n",
    "    return model\n",
    "modle=load_checkpoint('checkoutpoints/checkpoint_epoch_10_lr_1e-6_dropout_0_warmupstep_0.5_seed_42.pkl',optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa2aec",
   "metadata": {},
   "source": [
    "# Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99076dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc) \n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(modle, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa2c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63f0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34a196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c175f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f48a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
